{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OCQEseC7CFjS"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGTULlk0CFi0"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhL7z-1vCFi7"
      },
      "source": [
        "## Introduction\n",
        "- Supervised learning: we have an output or label to try to fit\n",
        "    - regression, classification\n",
        "- Unsupervised learning\n",
        "    - Clustering, PCA, MDS, SNE\n",
        "- **Clustering** is another often unsupervised learning method\n",
        "- Purpose: identify groups of data items that are similar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ8pUpgPCFi7"
      },
      "source": [
        "- In PCA/MDS etc, we have relied on visualization to find structure\n",
        "    - e.g., we check that similar items, such as cereals with high fibers, are indeed closeby in the embedding, but far from unsimilar items such as rice crisps\n",
        "    - visually similar items may form a group\n",
        "- In clustering, instead of visually identify groups, we want to find them **automatically**, using algorithms\n",
        "    - we do not want to project to lower dimension then do clustering, because embedding lose information\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVvLsCoUCFi8"
      },
      "source": [
        "## What is clustering?\n",
        "\n",
        "- **grouping** data items such that items in the same group are more **similar** to each other than to those in other groups \n",
        "- items in the same group form a **cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEeHWJGlCFi8"
      },
      "source": [
        "- e.g., based on the data from the left, most people would think that that there are 4 clusters\n",
        "- Why? Because perceptual \"similarity\"\n",
        "\n",
        "<img width=\"180px\" src=\"http://fengmai.net/download/data/bia652/images/cluster_bw.png\" style=\"float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\"></img><img width=\"180px\" src=\"http://fengmai.net/download/data/bia652/images/cluster_c.png\" style=\"float: right; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bciGELaSCFi8"
      },
      "source": [
        "## Similarity ##\n",
        "- **similar** is defined in some sense \n",
        "- Similarity may be subjective\n",
        "- Often, similarity is defined by a distance metric\n",
        "- Euclidean distance: \n",
        "\n",
        "$$||\\{x_1,x_2,x_3,\\ldots, x_n\\} - \\{y_1,y_2,y_3,\\ldots, y_n\\}||_2 = \\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\\ldots+(x_n-y_n)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETV2rrGuCFi9"
      },
      "source": [
        "- Or more generally, Minkowski distance\n",
        "$$\\left\\|\\{x_1,x_2,x_3,\\ldots, x_n\\} - \\{y_1,y_2,y_3,\\ldots, y_n\\}\\right\\|_p = \\left(|x_1-y_1|^p+|x_2-y_2|^p+\\ldots+|x_n-y_n|^p\\right)^{1/p}$$\n",
        "- $p=1$ gives the Manhattan distance\n",
        "$$\\left\\|\\{x_1,x_2,x_3,\\ldots, x_n\\} - \\{y_1,y_2,y_3,\\ldots, y_n\\}\\right\\|_1 = |x_1-y_1|+|x_2-y_2|+\\ldots+|x_n-y_n|$$\n",
        "- $p=\\infty$ gives\n",
        "$$\\left\\|\\{x_1,x_2,x_3,\\ldots, x_n\\} - \\{y_1,y_2,y_3,\\ldots, y_n\\}\\right\\|_\\infty = \\max\\{|x_1-y_1|, |x_2-y_2|, \\ldots, |x_n-y_n|\\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy5pHTz-CFi9"
      },
      "source": [
        "* [Other distance metrics](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOaIF9FVCFi9"
      },
      "source": [
        "## Types of clustering\n",
        "- exclusive/non-overlapping clustering\n",
        "    - each data item only belongs to 1 cluster \n",
        "- overlapping clustering\n",
        "    - a data item may belong to more than 1 cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooq1Bp_OCFi-"
      },
      "source": [
        "## Types of clustering methods\n",
        "- Hierarchical Clustering\n",
        "    - Divisive (top down): e.g., spectral clustering\n",
        "    - Agglomerative (bottom up): e.g., single linkage clustering, complex linkage clustering\n",
        "- Centroid-based clustering\n",
        "    - data items belongs to the closest center. Example: k-mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWYE8YxhCFi-"
      },
      "source": [
        "- Distribution-based (overlapping) clustering\n",
        "    - Based on statistical model\n",
        "    - E.g., Gaussian mixture model\n",
        "- Density-based clustering\n",
        "    - clusters are defined as areas of higher density than the remainder of the data set. \n",
        "    - e.g., DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x155dlu4CFi-"
      },
      "source": [
        "## Hierarchical clustering: agglomerative method \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkQSzESwCFi_"
      },
      "source": [
        "- The agglomerative method works bottom up\n",
        "- Every data item is assumes to be in its own cluster\n",
        "- At every step, choose two clusters that are **closest** and merge into a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIwpPwQQCFi_"
      },
      "source": [
        "- Example:\n",
        "\n",
        "<img src=\"http://fengmai.net/download/data/bia652/images/cluster_agg1.png\" style=\"float: left; width: 45%; margin-right: 1%; margin-bottom: 0.5em;\"></img>\n",
        "<img src=\"http://fengmai.net/download/data/bia652/images/cluster_agg2.png\" style=\"float: right; width: 45%; margin-right: 1%; margin-bottom: 0.5em;\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr0RQ6xxCFi_"
      },
      "source": [
        "- Need to define the distance between two clusters\n",
        "\n",
        "- *Complete-linkage clustering*: maximum distance between elements of each cluster\n",
        "    \n",
        "$$ \\text{dist}(A, B) = \\max_{x\\in A, y \\in B}\\ d(x,y) $$\n",
        "<img src=\"http://fengmai.net/download/data/bia652/images/cluster_dists.png\" style=\"float: right; width: 40%\">\n",
        "\n",
        "\n",
        "- *Single-linkage clustering*: minimum distance between elements of each cluster\n",
        "\n",
        "$$ \\text{dist}(A, B) = \\min_{x\\in A, y \\in B}\\ d(x,y) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK9IuLs3CFi_"
      },
      "source": [
        "- *Average linkage clustering*: mean distance between elements of each cluster\n",
        "$$ \\text{dist}(A, B) = \\frac{1}{|A||B|}\\sum_{x\\in A,\\ y \\in B}\\ d(x,y) $$\n",
        "\n",
        "- *Centroid linkage clustering*: distance between cluster is that between centroids\n",
        "$$ \\text{dist}(A, B) = d(\\mu(A),\\mu(B)) $$\n",
        "\n",
        "- *Ward linkage clustering*: distance between cluster is the variance of the union\n",
        "$$ \\text{dist}(A, B) = \\sigma^2(A\\cup B) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dayoph_eCFjA"
      },
      "source": [
        "- Difference: single linkage can give long chains as \"clusters\"\n",
        "- The other linkages all tends to avoid that\n",
        "- Example: consider applying single vs complete linkage to the following"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAiiAcisCFjA"
      },
      "source": [
        "from sklearn import cluster\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Yd2WyXCFjB"
      },
      "source": [
        "plt.scatter([0,1,2.1,3.3,4.6],[0,0,0,0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzswpbhtCFjB"
      },
      "source": [
        "- Using Python ```sklearn``` package ```cluster```\n",
        "- ```cluster.AgglomerativeClustering``` only allows ward, average, complete linkage, not single linkage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRzoytQCCFjB"
      },
      "source": [
        "ring = pd.read_csv(\"http://fengmai.net/download/data/bia652/images/clustering_example1.csv\",header=None).values\n",
        "# cluster.AgglomerativeClustering only allows ward, average, complete linkage\n",
        "ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
        "cluster_ward = ward.fit_predict(ring)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(ring[:,0], ring[:,1], c=cluster_ward, cmap='prism')  # plot points with cluster dependent colors\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdA4QLxlCFjC"
      },
      "source": [
        "- to get single linkage clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MN5sx2VHCFjC"
      },
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "Z = linkage(ring, 'single')\n",
        "n_clusters = 2\n",
        "clusters_single = fcluster(Z, n_clusters, criterion='maxclust')\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(ring[:,0], ring[:,1], c=clusters_single, cmap='prism')  # plot points with cluster dependent colors\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V1iiXPXCFjC"
      },
      "source": [
        "- Applied to cereal data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmidmMbCFjC"
      },
      "source": [
        "cereal = pd.read_csv(\"http://fengmai.net/download/data/bia652/images/cereal.csv\", sep=\" \")\n",
        "names = cereal[\"name\"].values\n",
        "cereal = cereal.drop([\"name\",\"mfr\",\"type\"],1)\n",
        "cereal[:6] # we can see that there are missing values! (-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ivMLg5sCFjC"
      },
      "source": [
        "- Stanardize and replace missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0_6EQkeCFjD"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "cereal2 = StandardScaler(with_std=True).fit_transform(cereal) #standardize\n",
        "for vals in cereal.columns:\n",
        "    c = cereal[vals]\n",
        "    avg = np.mean(c[c != -1])\n",
        "    cereal[vals] = c.replace(-1, avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT9DUPhhCFjD"
      },
      "source": [
        "- Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMg3hWKRCFjD"
      },
      "source": [
        "from sklearn import manifold\n",
        "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=123,\n",
        "                   dissimilarity=\"euclidean\", n_jobs=1)\n",
        "cereal_mds2 = mds.fit(cereal2).embedding_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OXp_Lg6CFjD"
      },
      "source": [
        "Z = linkage(cereal_mds2, 'single')\n",
        "n_clusters = 4\n",
        "cluster_single = fcluster(Z, n_clusters, criterion='maxclust')\n",
        "# colors\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "color = colors[cluster_single]\n",
        "#scatter plot\n",
        "plt.figure(figsize=(25, 10))\n",
        "for x, y, c in zip(cereal_mds2[:,0], cereal_mds2[:,1], color):\n",
        "    plt.scatter(x,y,color=c)\n",
        "#labels\n",
        "for label, x, y, c in zip(names, cereal_mds2[:,0],cereal_mds2[:,1], color):\n",
        "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
        "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtQSY5aZCFjD"
      },
      "source": [
        "- Try \"complete\", \"ward\" and \"average\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nszfej56CFjD"
      },
      "source": [
        "## Centroid-based clustering: k-mean clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQd0DWCPCFjE"
      },
      "source": [
        "- k-means partitions data items into $k$ clusters\n",
        "- It minimize the within-cluster sum of squares to the centers\n",
        "$$\\arg\\min_{s}\\sum_{i=1}^k \\sum_{x\\in S_i}\\|x - \\mu_i\\|^2$$\n",
        "- Where $\\mu_i$ is the mean of the data items in cluster $s_i$ and $S$ is the $k$ clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVcrmVPpCFjE"
      },
      "source": [
        "-  **Lloyd's algorithm**\n",
        "- Initialization: set $k$-centers $\\mu_j$\n",
        "- Assignment step: assign each data item to the closest center \n",
        "$$S_i = \\left\\{x_j\\ |\\ \\|x_j - \\mu_i\\| \\le \\|x_j - \\mu_l\\|,\\ 1\\le l\\le k\\right\\}$$\n",
        "- Update step: calculate the new means \n",
        "$$\\mu_i = \\frac{1}{|S_i|}\\sum_{x_j\\in S_i} x_j$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXEQZi7YCFjE"
      },
      "source": [
        "- Python package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3pr22gUiCFjE"
      },
      "source": [
        "from sklearn import cluster\n",
        "cluster_model = cluster.KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300, tol=0.0001)\n",
        "cluster_kmean = cluster_model.fit_predict(ring)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(ring[:,0], ring[:,1], c=cluster_kmean, cmap='prism')  # plot points with cluster dependent colors\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kbi5GN7bCFjE"
      },
      "source": [
        "from sklearn import cluster\n",
        "cluster_model = cluster.KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, tol=0.0001)\n",
        "cluster_kmean = cluster_model.fit_predict(cereal2)\n",
        "\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "color = colors[cluster_kmean]\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "#scatter plot\n",
        "for x, y, c in zip(cereal_mds2[:,0], cereal_mds2[:,1], color):\n",
        "    plt.scatter(x,y,color=c)\n",
        "#labels\n",
        "for label, x, y, c in zip(names, cereal_mds2[:,0],cereal_mds2[:,1], color):\n",
        "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
        "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_kSLSuGCFjF"
      },
      "source": [
        "- Convergence: the sum of square distances (SSD) to the cluster means never increases.  \n",
        "Assignment Step: each data assigned to a closest center, thus $\\|x-c_i\\|$ has to decrease, so is the SSD  \n",
        "Update Step: $\\mu_i=$ min least square, so SSD decreases\n",
        "- Hence the process will converge to local mininum. But local minimum can be quite bad (see fig).\n",
        "<img width=\"400px\" src=\"http://fengmai.net/download/data/bia652/images/cluster_localmin.png\" style=\"float: right; width: 40%\"></img>\n",
        "- Initialization is important\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Uscy55CFjF"
      },
      "source": [
        "- How best to initialize? Many suggestions.\n",
        "- k-means++: find the $k$ centers that spread apart\n",
        " - Choose one center uniformly at random from among the data points.\n",
        " - For each data point $x$, compute $\\min_{c\\in C} d(x, c)$ among all chosen centers $c$\n",
        " - Choose a $x$ as the center with probablity proportional to $\\min_{c\\in C} d(x, c)$\n",
        " - Repeat till $k$ centers have been chosen.\n",
        "- k-means++ algorithm guarantees an approximation ratio O(log k) in expectation over the global optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfZg_GCPCFjF"
      },
      "source": [
        "- How best to pick $k$?\n",
        "    - the sum of square distances usually go down as $k$ increases\n",
        "    - when $k = n$, it becomes zero!\n",
        "    - Can look at how it changes with $k$, and pick $k$ where the sum of square distances change slows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DACDlQLLCFjF"
      },
      "source": [
        "cluster_model.score(cereal2) # Opposite of the value of X on the K-means objective."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lg5NEuiCFjG"
      },
      "source": [
        "cluster_model = cluster.KMeans(n_clusters=70, init='k-means++', n_init=10, max_iter=300, tol=0.0001)\n",
        "cluster_kmean = cluster_model.fit_predict(cereal2)\n",
        "cluster_model.score(cereal2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_TB4oeCFjG"
      },
      "source": [
        "## Distribution-based clustering: Gaussian mixture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m49GUob_CFjG"
      },
      "source": [
        "### Gaussian mixture clustering algorithm\n",
        "- 1) Give an initial guess of $\\mu_j$ and $\\sigma_j$ for each of the $k$ clusters $c_j$, as well as proporation of items in each cluster, $p(c_j)$\n",
        "- 2) Compute the probability of each data point belonging to each of the $k$ clusters $c_j$\n",
        "$$p(c_j|x_i) = \\frac{p(x_i|c_j) p(c_j)}{\\sum_{l=1}^k p(x_i|c_l) p(c_l)}$$\n",
        " Here (note that here we are using the density function as if it is a probability mass function for discrete random variables)\n",
        "$$p(x_i|c_l) = \\frac{1}{\\sqrt{2\\sigma_l^2\\pi}}e^{-\\frac{(x_i-\\mu_l)^2}{2\\sigma_l^2}}$$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pfr9wHFCFjG"
      },
      "source": [
        "- 3) Re-estimate mean, std\n",
        " $$\\mu_j = \\frac{\\sum_{i=1}^n p(c_j|x_i) x_i}{\\sum_{i=1}^n p(c_j|x_i)},\\ \\sigma_j = \\frac{\\sum_{i=1}^n p(c_j|x_i) (x_i - \\mu_j)^2}{\\sum_{i=1}^n p(c_j|x_i)}$$\n",
        "- 4) Estimate the number of points in each cluster\n",
        "$$\\text{#points in cluster }j = \\sum_{i=1}^n p(x_i|c_j)$$\n",
        "Probability any random data point belongs to cluster $j$ is\n",
        "$$p(c_j) = \\frac{\\sum_{i=1}^n p(x_i|c_j)}{n}$$\n",
        "- 5) Repeat from 2. till the mean and std of the clusters stop changing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11pfYwRCCFjH"
      },
      "source": [
        "- For multidimensional data, $x_i$, $\\mu_j$ are vectors, $\\Sigma_j$ is a covariance matrix\n",
        "- Estimation becomes \n",
        "$$\\Sigma_j = \\frac{\\sum_{i=1}^n p(c_j|x_i) (x_i - \\mu_j)(x_i - \\mu_j)^T}{\\sum_{i=1}^n p(c_j|x_i)}$$\n",
        "- \n",
        "$$p(x_i|c_l) = \\frac{1}{\\sqrt{(2\\pi)^k|\\Sigma_l|^2}}e^{-\\frac{1}{2} (x-\\mu_l)^T\\Sigma_j(x-\\mu_l)}$$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGYuZat_CFjI"
      },
      "source": [
        "- One can use to generate overlapping clusters (i.e, a point may be p=0.6 in cluster 1 and p=0.4 in cluster 2)\n",
        "- Or use thresholding to do hard non-overlapping clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmjjz_a8CFjJ"
      },
      "source": [
        "- Let's apply to the ring example: unfortunately Gaussian Mixture clustering assume ellipsoid shaped clusters, does not work well for the ring shape cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lQhVoj1DCFjJ"
      },
      "source": [
        "from sklearn import mixture\n",
        "\n",
        "gmm = mixture.GaussianMixture(n_components=2, covariance_type = 'full', random_state=123)\n",
        "\n",
        "gmm.fit(ring)\n",
        "cluster_gmm = gmm.predict(ring)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(ring[:,0], ring[:,1], c=cluster_gmm, cmap='prism')  # plot points with cluster dependent colors\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMMIx52qCFjJ"
      },
      "source": [
        "gmm.bic(ring)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m06I1JT7CFjN"
      },
      "source": [
        "- To cereal data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_MNjoy6CFjN"
      },
      "source": [
        "from sklearn import mixture\n",
        "gmm = mixture.GaussianMixture(n_components=4, random_state=123)\n",
        "gmm.fit(cereal2)\n",
        "cluster_gmm = gmm.predict(cereal2)\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "color = colors[cluster_gmm]\n",
        "plt.figure(figsize=(25, 10))\n",
        "#scatter plot\n",
        "for x, y, c in zip(cereal_mds2[:,0], cereal_mds2[:,1], color):\n",
        "    plt.scatter(x,y,color=c)\n",
        "#labels\n",
        "for label, x, y, c in zip(names, cereal_mds2[:,0],cereal_mds2[:,1], color):\n",
        "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
        "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-ZoUqQCFjO"
      },
      "source": [
        "## Density-based clustering: DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0tVC-IBCFjO"
      },
      "source": [
        "- This is a density based method\n",
        "- It does not require the user to specify $k$ - the number of clusters\n",
        "- The basic idea is to start from a point with sufficient number of neighbors (called a core point), and keep growing by including all the $\\epsilon$-neighbors, and the neighbors of the core neighbors. \n",
        "- Take only two parameters: $\\epsilon$ and $t$ (min number of neighbors), and a distance measure $d$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsBloBJSCFjP"
      },
      "source": [
        "- Let $X$ be the set of $n$ data points.\n",
        "- Let $N(x,\\epsilon)$ be the data items within a radius of $\\epsilon$ from $x$\n",
        "$$N(x,\\epsilon) = \\{y| d(x,y) \\le \\epsilon,\\ y \\in X\\}$$\n",
        "- $|N(x,\\epsilon)|$ is the number of neighbors\n",
        "- If $N(x, \\epsilon) \\ge t$, $x$ is called a core point, else it is called a bounary point.\n",
        "- DBSCAN iteratively connects two core points if they can be reached through another point within the radius.\n",
        "- DBSCAN allow points to be noise (not in any cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-AUh2wPCFjQ"
      },
      "source": [
        "- Using Python sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsfYAT2WCFjQ"
      },
      "source": [
        "from sklearn import cluster\n",
        "# try 0.4, 0.3, ...\n",
        "dbscan = cluster.DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
        "cluster_dbscan = dbscan.fit_predict(ring)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(ring[:,0], ring[:,1], c=cluster_dbscan, cmap='prism');  # plot points with cluster dependent colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgCdDqPxCFjR"
      },
      "source": [
        "- On cereal data: has to be careful with $\\epsilon$, also misses the \"Bran\" cluster at the top"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uUshLGOgCFjR"
      },
      "source": [
        "from sklearn import cluster\n",
        "# have to adjust eps carefully to get reasonable clusters\n",
        "dbscan = cluster.DBSCAN(eps=2, min_samples=5, metric='euclidean')\n",
        "cluster_dbscan = dbscan.fit_predict(cereal2)\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "color = colors[cluster_dbscan]\n",
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "#scatter plot\n",
        "for x, y, c in zip(cereal_mds2[:,0], cereal_mds2[:,1], color):\n",
        "    plt.scatter(x,y,color=c)\n",
        "#labels\n",
        "for label, x, y, c in zip(names, cereal_mds2[:,0],cereal_mds2[:,1], color):\n",
        "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
        "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYq4car3CFjS"
      },
      "source": [
        "- Advantage: fast ($O(n log(n))$ operations using quad-tree; no need to specify $k$\n",
        "- Disadvantage: sensitive to $\\epsilon$ and distance measure\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmt68lJHCFjS"
      },
      "source": [
        "twoball = pd.read_csv(\"http://fengmai.net/download/data/bia652/images/cluster_dbscan_prob.csv\", header=None).values\n",
        "from sklearn import cluster\n",
        "dbscan = cluster.DBSCAN(eps=0.1, min_samples=5, metric='euclidean')\n",
        "cluster_dbscan = dbscan.fit_predict(twoball)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(twoball[:,0], twoball[:,1], c=cluster_dbscan, cmap='prism');  # plot points with cluster dependent colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCQEseC7CFjS"
      },
      "source": [
        "## Hierarchical clustering/divisive method\n",
        "<img src=\"http://fengmai.net/download/data/bia652/images/cluster_divisive.png\" width=\"300px\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OspE3fWpCFjT"
      },
      "source": [
        "## Spectral clustering for graphs\n",
        "- To explain spectral clustering, we first have to talk about **graph clustering**\n",
        "- Consider a connected graph of vertices $V$ and edges $E$. There are $|V|$ vertices and $|E|$ edges.\n",
        "- Each edge $\\{i,j\\}$ has a weight $w_{ij}$\n",
        "- The sum of weights for each node $i$ is denoted as $d_i = \\sum_{j\\in V,\\ j\\ne i} w_{ij}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcWwE3JmCFjT"
      },
      "source": [
        "- We want to partition nodes of a graph into clusters so that the sum of edge weights within cluster is large, and sum of edge weights between clusters is small\n",
        "- Spectral clustering is one way to cluster graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_5L6B5CFjT"
      },
      "source": [
        "- Suppose we partitioned vertices into two clusters $A$ (red) and $B$ (blue)\n",
        "- We want to minimize the **cut**: the sum of $w_{ij}$ with $i\\in A$ and $j \\in B$.\n",
        "<img width=\"700px\" src=\"http://fengmai.net/download/data/bia652/images/cluster_spectral.png\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3MzrkfZCFjT"
      },
      "source": [
        "- We have to make sure the two clusters are balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAVysWNpCFjT"
      },
      "source": [
        "- We define normalized cut:\n",
        "$$\\text{Ncut}(A, B) = \\frac{\\text{cut}(A, B)}{\\text{vol}(A)} + \\frac{\\text{cut}(B, A)}{\\text{vol}(B)}$$\n",
        "- Here $\\text{vol}(A)$ is the sum of weights of all edges with one end in $A$.\n",
        "$$\\text{vol}(A) = \\sum_{i\\in A} d_i,\\ \\text{vol}(B) =  \\sum_{i\\in B} d_i$$\n",
        "- This makes sure that $vol(A)$ and $vol(B)$ are not too small, otherwise $\\text{Ncut}(A, B)$ would be large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nCVn5SCCFjU"
      },
      "source": [
        "### Spectral Clustering for multidimensional data\n",
        "- To apply spectral clustering to multidimensional data, we need to establish a similarity graph\n",
        "- Possibilities: nearest neighbor graph; or similarity graph via Gaussian kernel\n",
        "- For example, similarity between two multidimensional vectors $i$ and $j$ is\n",
        "$$e^{-\\frac{\\|x_i-x_j\\|}{2\\sigma_i^2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_uciTOECFjU"
      },
      "source": [
        "- This is known as the Gaussian kernel similarity function\n",
        "- Here $\\sigma$ controls the size of the neighborhood\n",
        "- Example: similarity graph (with cut off threshold similarity = 0.1) for different $\\sigma$\n",
        "<img src=\"http://fengmai.net/download/data/bia652/images/cluster_spectral_gaussian.png\" width=\"600px\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqcb-iTICFjU"
      },
      "source": [
        "- Using Python package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJMvZAlCCFjU"
      },
      "source": [
        "#import data\n",
        "ring = pd.read_csv(\"http://fengmai.net/download/data/bia652/images/clustering_example1.csv\",header=None).values\n",
        "plt.scatter(ring[:,0], ring[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGUHvhU4CFjU"
      },
      "source": [
        "from sklearn import cluster\n",
        "#Using nearest neighbor for similarity graph\n",
        "spectral = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack', affinity=\"nearest_neighbors\")\n",
        "clu = spectral.fit_predict(ring)\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "plt.scatter(ring[:,0], ring[:,1], color=colors[clu])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gppe1dnOCFjV"
      },
      "source": [
        "#Using Gaussian kernel we define ourselves\n",
        "sigma=0.2\n",
        "def sim(x, y):\n",
        "    return np.exp(-((x[0]-y[0])**2+(x[1]-y[1])**2)/(2.*sigma**2))\n",
        "spectral = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack', affinity=sim)\n",
        "clu = spectral.fit_predict(ring)\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "plt.scatter(ring[:,0], ring[:,1], color=colors[clu])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIKiyG_5CFjV"
      },
      "source": [
        "### Apply spectral clustering to cereal data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg-eIO56CFjV"
      },
      "source": [
        "cereal = pd.read_csv(\"http://fengmai.net/download/data/bia652/images/cereal.csv\", sep=\" \")\n",
        "names = cereal[\"name\"].values\n",
        "cereal = cereal.drop([\"name\",\"mfr\",\"type\"],1)\n",
        "cereal[:6] # we can see that there are missing values! (-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hOB_V-7CFjV"
      },
      "source": [
        "- We replace missing values with the mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dTbWZR1CFjV"
      },
      "source": [
        "for vals in cereal.columns:\n",
        "    c = cereal[vals]\n",
        "    avg = np.mean(c[c != -1])\n",
        "    cereal[vals] = c.replace(-1, avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi3HDSu1CFjW"
      },
      "source": [
        "- Spectral clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "smMcaxERCFjW"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "cereal2 = StandardScaler(with_std=True).fit_transform(cereal) #standardize\n",
        "\n",
        "from sklearn import cluster\n",
        "#Using nearest neighbor for similarity graph\n",
        "spectral = cluster.SpectralClustering(n_clusters=4, eigen_solver='arpack', affinity=\"nearest_neighbors\")\n",
        "clu = spectral.fit_predict(cereal2)\n",
        "np.transpose([names, clu])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEUXgo5GCFjW"
      },
      "source": [
        "- Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UTeu1c5bCFjW"
      },
      "source": [
        "from sklearn import manifold\n",
        "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=123,\n",
        "                   dissimilarity=\"euclidean\", n_jobs=1)\n",
        "cereal_mds2 = mds.fit(cereal2).embedding_\n",
        "\n",
        "# colors\n",
        "colors = np.array([x for x in 'bgrcmyk'])\n",
        "color = colors[clu]\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "#scatter plot\n",
        "for x, y, c in zip(cereal_mds2[:,0], cereal_mds2[:,1], color):\n",
        "    plt.scatter(x,y,color=c)\n",
        "#labels\n",
        "for label, x, y, c in zip(names, cereal_mds2[:,0],cereal_mds2[:,1], color):\n",
        "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
        "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJU3azJsCFjW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}