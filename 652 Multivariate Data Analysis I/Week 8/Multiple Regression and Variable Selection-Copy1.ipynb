{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XMhARnJKYOB0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: setuptools in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from mlxtend) (3.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: six in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from pandas>=0.24.2->mlxtend) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: statsmodels in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: scipy>=1.3 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from statsmodels) (1.7.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from statsmodels) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from statsmodels) (22.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from statsmodels) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from pandas>=0.25->statsmodels) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: six in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/apple/opt/anaconda3/envs/py3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U mlxtend\n",
    "! pip install -U statsmodels\n",
    "! pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eI-xcBTqYOB6"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/16/sj7htwpn37x5chbjt67h8nrr0000gn/T/ipykernel_54854/4093800386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the Boston housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mboston\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_boston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \"\"\"\n\u001b[1;32m    155\u001b[0m         )\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from tqdm.auto import tqdm\n",
    "# Load the Boston housing dataset\n",
    "boston=datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_YbBCVPYOB9"
   },
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoeYq5x5YOB-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RG1YXdeYOCE"
   },
   "outputs": [],
   "source": [
    "boston_data = pd.DataFrame(boston.data)\n",
    "boston_data.columns = boston['feature_names']\n",
    "boston_data['MEDV'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlB4P6dVYOCH"
   },
   "outputs": [],
   "source": [
    "boston_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dk_kdMXUYOCL"
   },
   "outputs": [],
   "source": [
    "boston_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDlTYkTUYOCO"
   },
   "source": [
    "Run a full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-IaOR5RYOCP"
   },
   "outputs": [],
   "source": [
    "y = boston_data[\"MEDV\"]\n",
    "X = boston_data.drop([\"MEDV\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hOZEN05YOCT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = sm.add_constant(X) # by default statsmodels does not add an intercept\n",
    "# add a constant is the same as adding a column of 1 to X .\n",
    "full_model = sm.OLS(y, X).fit()\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlZ14FklYOCX"
   },
   "source": [
    "It might be easier to use R-style formulas when there are only a few variables. You do not need to explicitly add an intercept term as in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMnCxBWgYOCY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2 = smf.ols(formula='MEDV ~ CRIM + ZN', data=boston_data)\n",
    "res = model_2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHblXDxAYOCb"
   },
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbax6WOFYOCc"
   },
   "source": [
    "Boostrap is a key tool in modern statistics to quantify the uncertainty of estimation. It is is a computer-intensive procedure that substitutes fast computation for theoretical math. The idea is quite simple.  \n",
    "\n",
    "1. Suppose you have a dataset (sample) and use it to find $\\hat{\\theta}$, your estimate the unknown parameter $\\theta$. \n",
    "2. Then draw a new random sample of size n, with replacement, from the orignial dataset. The sample size n is the same as the size of the original dataset (sample). \n",
    "3. This new sample is called a bootstrap sample. For this bootstrap sample, we can calculate a new estimate $\\hat{\\theta}_1$. \n",
    "4. Repeat step 2 and step 3 $K$ times and get $\\hat{\\theta}_1$, $\\hat{\\theta}_2$,...,$\\hat{\\theta}_K$.\n",
    "5. The spread in these estimates tells us how large the estimation error is. Suppose we want to set a 95% confidence interval on $\\theta$, the true parameter value. And suppose we take K = 5000 bootstrap samples. The bootstrap theory suggests that approximately 95% of the time, the true parameter value falls between the 2.5th percentile of the bootstrap samples (or the 125 smallest out of 5000) and the 97.5th percentile (or the 125 largest). As such, the 2.5th percentile of $\\hat{\\theta}_1$, $\\hat{\\theta}_2$,...,$\\hat{\\theta}_{5000}$ and the 97.5th percentile provides the 95% CI for $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4dU0AJfYOCc"
   },
   "outputs": [],
   "source": [
    "bootstrapped_ests = []\n",
    "for i in tqdm(range(1000)):\n",
    "    boston_data_boot = boston_data.sample(n=len(boston_data), replace=True)\n",
    "    boston_model_boot = smf.ols(formula='MEDV ~ CRIM + ZN', data=boston_data_boot).fit()\n",
    "    bootstrapped_ests.append(boston_model_boot.params[['CRIM', 'ZN']])\n",
    "\n",
    "b_CRIMs, b_ZNs = zip(*bootstrapped_ests)\n",
    "\n",
    "print(\"The 95% bootstrapped CI of b_CRIMs is [{:.2f}, {:.2f}].\".format(\n",
    "    np.percentile(b_CRIMs, 2.5),\n",
    "    np.percentile(b_CRIMs, 97.5)))\n",
    "\n",
    "print(\"The 95% bootstrapped CI of b_ZNs is [{:.2f}, {:.2f}].\".format(\n",
    "    np.percentile(b_ZNs, 2.5),\n",
    "    np.percentile(b_ZNs, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwi3HSRHYOCf"
   },
   "source": [
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g5vt4BJYOCf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Iw2QXFEYOCi"
   },
   "source": [
    "## sklearn model without selection  \n",
    "[Read the manual](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA8zHYoWYOCj"
   },
   "outputs": [],
   "source": [
    "model_3 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGGDRTRLYOCn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_3.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YE-JyPtvYOCs",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_3.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd5EhD9YOCv"
   },
   "source": [
    "## Stepwise Regression using sklearn + mlxtend.  \n",
    "[Read the manual and examples](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGKFE2xaYOCw"
   },
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "model_sfs = SFS(model_3, scoring='neg_mean_squared_error', k_features=4, verbose=1, cv=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PvB-4OSYOCy"
   },
   "outputs": [],
   "source": [
    "model_sfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmI66FqYYOC2"
   },
   "outputs": [],
   "source": [
    "model_sfs.get_metric_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TReqZ8XFYOC5"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(model_sfs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRnI1zOiYOC8"
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1 = plot_sfs(model_sfs.get_metric_dict(), kind='std_dev')\n",
    "plt.title('Sequential Forward Selection')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvM-CJkbYOC-"
   },
   "source": [
    "After selection, we need to refit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxEQk4rGYOC_"
   },
   "outputs": [],
   "source": [
    "X_selected = model_sfs.transform(X)\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9MW71YKYODB"
   },
   "outputs": [],
   "source": [
    "model_3_after_selection = model_3.fit(X_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOcr8H3HYODD"
   },
   "source": [
    "We can use the refitted model to make predictions (in-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cVpBscFYODD"
   },
   "outputs": [],
   "source": [
    "model_3_after_selection.predict(X_selected)[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UuD9X_YYODH"
   },
   "source": [
    "## Recursive feature elimination with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "casSCMuSYODH"
   },
   "source": [
    "[Manual](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U16HF4KYODI"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW_N8iyGYODL"
   },
   "outputs": [],
   "source": [
    "model_RFE = RFE(model_3, n_features_to_select=4)\n",
    "model_RFE.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2sTPDKjYODO"
   },
   "outputs": [],
   "source": [
    "model_RFE.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KF1Kt3rYODR"
   },
   "source": [
    "Get the selected X variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "343-jkHkYODS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.loc[:, model_RFE.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML4DFRmTYODU"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMus-hP9YODU"
   },
   "source": [
    "[Lasso Manual](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)  \n",
    "[LassoCV Manual](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SccMKFcCYODV"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV, lars_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ijl_FFHYODX"
   },
   "source": [
    "We can set the alpha ($\\lambda$ in slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNKIePO6YODX"
   },
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha = 0.1, normalize=True)\n",
    "model_lasso.fit(X, y)\n",
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZewviUWYODa"
   },
   "source": [
    "Which variables are selected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbUIDQMvYODb"
   },
   "outputs": [],
   "source": [
    "X.columns[np.abs(model_lasso.coef_) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYIcd1T2YODc"
   },
   "source": [
    "LassoCV can search for best alpha automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_55zkbdeYODd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lassoCV = LassoCV(cv=5, normalize=True)\n",
    "model_lassoCV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiuE7se6YODf"
   },
   "outputs": [],
   "source": [
    "model_lassoCV.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2V09xUuYODh"
   },
   "source": [
    "We can also visualize the Lasso Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3-pu1aqYODi"
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36Vv4QnlYODk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphas_lasso, coefs_lasso, _ = model_lasso.path(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSGV1U7yYODm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, _, coefs = lars_path(X.values, y.values.flatten(), method='lasso')\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8o3Spg7YODo"
   },
   "source": [
    "# Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWagkbBMYODo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMiyRn_MYODq"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADylII41YODu"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdlwBuWtYOD0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK0RpQeuYOD3"
   },
   "source": [
    "**Important**: Standardizing/Normalizing data is part of the model training process. You shoud fit a standardizer (learning the mean and std from the train set) and use it to transform both the train and test set. See [here](https://scikit-learn.org/stable/modules/preprocessing.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_0w653UYOD4"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w8Qcmm_YOD6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lassoCV = LassoCV(cv=5, normalize=False) # note we don't need to normalize again\n",
    "model_lassoCV.fit(scaler.transform(X_train), y_train) # note we transform X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqL-4k4LYOEA"
   },
   "source": [
    "Predict test set and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSRpVLkPYOEA"
   },
   "outputs": [],
   "source": [
    "y_hat_test = model_lassoCV.predict(scaler.transform((X_test))) \n",
    "# note we transform X_test using the scaler learned from the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clSBD456YOED"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omB6lsrsYOEG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_true=y_test, y_pred=y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBEwNZw3HCZa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1yvnKMWtncLtKSvle7rb_v_UC3w3nbVJy",
     "timestamp": 1667254304459
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
