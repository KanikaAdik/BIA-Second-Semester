{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di90tqslIeo7"
   },
   "source": [
    "## Simple Linear Regression ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_DJ3IzJIeo7"
   },
   "source": [
    "### Introduction ###\n",
    "- Suppose we have two measurements for each individual, and we have $n$ individual.\n",
    "- One of the measurement is a response variable $y_i,\\ i=1,2,\\ldots, n$.\n",
    "- We have $1$ explanatory variables $x_{i}, i =1,2,\\ldots, n$\n",
    "- We assume a linear relation between them\n",
    "$$y_i =  \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrdemfM6Ieo8"
   },
   "source": [
    "- $\\epsilon_i$ are Normal disturbance terms, e.g., due to measurement error\n",
    "- $\\epsilon_i$ is the only source of the randomness that we care about. Since we are interested in $p(Y|X)$, we can assume $X$ has an arbitrary distribution or non-random. \n",
    "- $\\beta_0$ and $\\beta_1$ are key parameters to be estimated. \n",
    "- each disturbance $\\epsilon_i$ has mean 0, and the same variance $\\sigma^2$\n",
    " \n",
    " <img src=\"https://fmai-teaching.s3.amazonaws.com/bia652/regression_assumption.jpg\" width=\"500px\"></img>\n",
    "\n",
    "$\\epsilon_i$ are independent from each other\n",
    "\n",
    "$$ cov(\\epsilon_i, \\epsilon_j)= \n",
    "\\left(\\begin{array}{cc}\n",
    " 0, & i \\ne j \\\\ \\sigma^2, & i = j \\\\ \n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imkI2v7IepN"
   },
   "source": [
    "###  Questions to ask ###\n",
    "\n",
    "- How to find the regression line (estimating $\\beta_1$ and $\\beta_0$)? \n",
    "- How can we quantify the uncertainty of the estimation/prediction?\n",
    "- How well does the regression line fit the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylV8aJS6IepO"
   },
   "source": [
    "### Ordinary Least Square (OLS) derivation ###\n",
    "- Here is one way to fit the regression line\n",
    "- Model: $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "- We want to minimize the square error, $f(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0  - \\beta_1 x_i)^2$\n",
    "- A qudratic function with 2 variables $\\beta_0$ and $\\beta_1$. Minimum achieved when the derivatives are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mlQDrsAIepP"
   },
   "source": [
    "- $\\frac{\\partial f}{\\partial\\beta_1}= 2 \\sum_{i=1}^{n}x_i (\\beta_0 + \\beta_1 x_i - y_i) = 0$\n",
    "    - $(\\sum_{i=1}^{n} x_i^2)\\beta_1 + (\\sum_{i=1}^{n} x_i)\\beta_0 = \\sum_{i=1}^{n}x_i y_i$ or $(\\sum_{i=1}^{n} x_i^2)\\beta_1 + n \\bar X \\beta_0 = \\sum_{i=1}^{n}x_i y_i$\n",
    "- $\\frac{\\partial f}{\\partial\\beta_0}= 2 \\sum_{i=1}^{n} (\\beta_0 + \\beta_1 x_i - y_i) = 0$\n",
    "    - $(\\sum_{i=1}^{n} x_i)\\beta_1 + n \\beta_0 = \\sum_{i=1}^{n} y_i$\n",
    "     or $n\\bar X \\beta_1 + n \\beta_0 = n\\bar Y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knTiewm7IepQ"
   },
   "source": [
    "- Thus \n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\hat\\beta_1 &=& \\frac{\\sum_{i=1}^{n}x_i y_i-n\\bar X \\bar Y }{ \\sum_{i=1}^{n} x_i^2-n {\\bar X}^2 }\n",
    "= \\frac{\\sum_{i=1}^{n}(x_i-\\bar X)(y_i-\\bar Y)}{\\sum_{i=1}^{n}(x_i-\\bar X)^2}\\\\\n",
    "\\hat\\beta_0 &=& \\bar Y - \\hat\\beta_1 \\bar X\n",
    "\\end{array}\n",
    "$$\n",
    "- Notice that we use $\\hat\\beta_i$ to denoted estimated parameters $\\beta_i$\n",
    "- Let $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i$ be the predicted value of $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7i4nz_FUmUr"
   },
   "source": [
    "### MLE derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-d8XECfUmUr"
   },
   "source": [
    "\n",
    "* **Likelihood function**: The probability that the data your observations arise from a specific probability distribution defined by a specific set of parameters.\n",
    "\n",
    "More succinctly, it is the likelihood of the data ($Y$) given the specific predictor variables ($X$) and a mapping fuction ($f()$), including the parameters that describe the distribution of the data. \n",
    "\n",
    "Now that last part of the description of the likelihood is the important part. This is why we have the  assumption that $\\epsilon_i$ is normally distributed. Remember that the probability distribution function for a normal distribution is\n",
    "\n",
    "$$ f(x | \\mu, \\sigma) = \\frac{1} {{\\sigma \\sqrt {2\\pi } }} e^{{\\frac{ - ( {x - \\mu })^2 }{2\\sigma^2} }} $$\n",
    "\n",
    "Now if we assume that $\\epsilon_i$ are normally distributed, $X$ is non-random, $\\beta_0$ and $\\beta_1$ are parameters (fixed numbers), it follows that $Y$ is normally distributed (what is its mean and standard deviation?). Thus we can assume that the likelihood is the product ($\\prod$) of all PDFs for $Y$s, which  are random variables from a normal distribution.\n",
    "\n",
    "\n",
    "$$ \\prod_{i=1}^{n} p(y_i | x_i; \\beta_0, \\beta_1, \\sigma) =  \\prod_{i=1}^{n} \\frac{1} {{\\sigma \\sqrt {2\\pi } }} e^{{\\frac{ - ( {y_i - (\\beta_0 + \\beta_1x_i) })^2 }{2\\sigma^2} }} $$\n",
    "\n",
    "In plain English, this says that the likelihood is the aggregated probability of observing a particular value of $y$, given the parameters we want to estimate. In this case we want to _maximize_ this function, such that the data has the highest probability of arising from a model with a specific set of values for $\\beta_0, \\beta_1,$ and $\\sigma$.\n",
    "\n",
    "In practice it is easier to take the log of this function, called the _log likelihood function_ ($logL$), which makes the problem boil down to more simple algebra.\n",
    "\n",
    "$$  logL(\\beta_0, \\beta_1, \\sigma)= \\log \\prod_{i=1}^{n} p(y_i | x_i; \\beta_0, \\beta_1, \\sigma) \\\\\n",
    "= \\sum_{i=1}^{n} \\log  p(y_i | x_i; \\beta_0, \\beta_1, \\sigma) \\\\ \n",
    "= \\frac{-n}{2} \\log(2\\pi) - n \\log(\\sigma) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2 \n",
    "$$\n",
    "\n",
    "It is clear that for any $\\sigma$, the object function for $\\beta$s is the same as the OLS objective function. \n",
    "\n",
    "<br>\n",
    "<center> <b> OLS estimators = MLE estimators! </b> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm1w8UgdIepR"
   },
   "source": [
    "### Estimation of the variance ###\n",
    "\n",
    "- Can we estimate the variance $\\sigma^2$ of $\\epsilon_i$?\n",
    "- The unbiased estimate of $\\sigma^2$ is call **residual mean square (RES. MS)** or **mean square error (MSE)** and is computed as\n",
    "$$S^2 = \\frac{\\sum_{i=1}^{n} (y_i-\\hat y_i)^2}{n-2}$$\n",
    "- This measures how far our prediction is from the observed\n",
    "\n",
    "    <img src=\"https://fmai-teaching.s3.amazonaws.com/bia652/se.jpg\"></img>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SU0-rcCIepR"
   },
   "source": [
    "- The estimate of $\\sigma^2$ is call **residual mean square (RES. MS)** or **mean square error (MSE)** and is computed as\n",
    "$$S^2 = \\frac{\\sum_{i=1}^{n} (y_i-\\hat y_i)^2}{n-2}$$\n",
    "- $n-2$ is the residual degree of freedom which is sample size - #parameters. Dividing by that instead of $n$ gives **unbiased** estimate of variance.\n",
    "- Why minus 2? Think what happen if $n=2$? You can fit perfectly! When $n=3$, really you only have one free point. Hence $n-2$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tG5RZl5IepU"
   },
   "source": [
    "### Hypothesis testing and confidence intervals for $\\hat\\beta_0$ and $\\hat\\beta_1$\n",
    "- Now that we have found the estimate parameters, sometimes we may wish to tell if, for example, whether one of them is really zero\n",
    "- The more general case: ** null hypothesis ** $H_0: \\beta_i = \\beta_i^0$\n",
    "- For example, we want to check whether $\\beta_1 = 0$ (i.e., $Y$ is not influenced by $X$ at all)\n",
    "- As usual, we need to estimate the mean and variance/standard deviation of $\\hat\\beta_0$ and $\\hat\\beta_1$\n",
    "- The ** standard errors ** $=$ uncertainty of using $\\hat\\beta_0$ and $\\hat\\beta_1$ to estimate $\\beta_0$ and $\\beta_1$\n",
    "    - $SE(\\hat\\beta_0) = S\\left(\\frac{1}{n}+\\frac{\\bar X^2}{\\sum_{i=1}^{n}(x_i-\\bar X)^2}\\right)^{1/2}$\n",
    "    - $SE(\\hat\\beta_1) = \\frac{S}{\\left(\\sum_{i=1}^{n}(x_i-\\bar X)^2\\right)^{1/2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS6rK850IepW"
   },
   "source": [
    "- The null hypothesis test statistics for $\\hat\\beta_i$ is $t_i=\\frac{\\hat\\beta_i - \\beta_i^0}{SE(\\hat\\beta_i)}$, applied to a $t$-distribution of degree $n-2$. \n",
    "- Consider it as the z-score when sample size is large!\n",
    "- It means that given the null hypothesis $H_0$, the probability that we observe the computed $\\hat\\beta_i$ = the part of the areas of the probability density function of a t-distribution outside of $t_i$\n",
    "- If the the test statistics (t) is large, or p-value is close to 0, evidence against null hypothesis is strong; if test statistics is small and p-value is large, evidence is weak. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaOokogoIepX"
   },
   "source": [
    "- e.g., if $\\hat\\beta_i = 40$, $\\beta_i^0 = 4$, $SE(\\hat\\beta_i) = 3$ and $n = 10$, $t_i = \\frac{40 - 4}{3} = 9$\n",
    "- $P(|x| \\ge 9) = P(x \\le -9) + P(x \\ge 9) =  CDF(-9)+(1-CDF(9)) = 1.85\\times 10^{-5}$\n",
    "- So null hypothesis is rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-ZxqN1NIepX"
   },
   "outputs": [],
   "source": [
    "df = 10 - 2\n",
    "stats.t.cdf(-9, df) + (1 - stats.t.cdf(9, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwYVheuIepb"
   },
   "source": [
    "- Confidence intervals for the parameters are\n",
    "    \n",
    "$CI(\\hat\\beta_i) = \\beta_i \\pm t(\\alpha) SE(\\hat\\beta_i)$\n",
    "\n",
    "It means that the probability of the real parameter values being outside of this interval is $\\alpha$\n",
    "\n",
    "$[-t(\\alpha),t(\\alpha)]$ is the $1-\\alpha$ confidence interval for $t$-distribution with $n-2$ degree of freedom.\n",
    "\n",
    "e.g., if n = 4, and we want to find the 95% confidence interval, then $\\alpha=0.05$, and we want to find $t(0.05)={CDF}^{-1}(1-0.05/2) = 4.3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJkNgWytIepc",
    "nbpresent": {
     "id": "892fdfc1-2fe4-45ff-8d03-e6cc9143e768"
    }
   },
   "outputs": [],
   "source": [
    "df = 4 - 2\n",
    "stats.t.ppf(1 - 0.05 / 2, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw-UrDhwIepe"
   },
   "source": [
    "### ANOVA (Analysis of variance) ###\n",
    "- The purpose is to test the **null hypothesis: $\\beta_1 = 0$*\n",
    "* ($y =$ constant)\n",
    "\n",
    "| Source |    SS (sum of square)    | df (degree of freedom)              | Mean Square | F |\n",
    "|:------:|:--------------------------:| :--: | :--------: | :-: |\n",
    "| Regression | ${ss}_{reg}= \\sum_{i=1}^n (\\hat y_i - \\bar Y)^2$ | $1$ | $\\frac{SS_{reg}}{1}$ | $\\frac{MS_{reg}}{MS_{res}}$ |\n",
    "| Residual |  ${ss}_{res}= \\sum_{i=1}^n (y_i - \\hat y_i)^2$     | $n-2$ | $\\frac{SS_{res}}{n-2}$ |   |\n",
    "| total    | ${ss}_{tot}=\\sum_{i=1}^n ( y_i - \\bar Y)^2 = {ss}_{reg}+{ss}_{res}$       | $n-1$  |       | | \n",
    "\n",
    "- $F = \\frac{MS_{reg}}{MS_{res}} = \\frac{SS_{reg}}{1}/\\frac{SS_{res}}{n-2}$ is the F statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_QimJFiIepf"
   },
   "source": [
    "- The F distribution is a right-skewed distribution used most commonly in Analysis of Variance. \n",
    "- When referencing the F distribution, the numerator degrees of freedom are always given first, so we use $F(1,n-2)$\n",
    "- The F value tests the null hypothesis that the regression coefficient $\\beta_1$ is zero, i.e., $y_i$ can be approximated by $\\bar Y$.\n",
    "- Large $F$ means that null hypothesis is not true (regression parameters nonzero)\n",
    "\n",
    "__When to use F-test v.s. t-test?__   \n",
    "- Equivalent when there is only one $x$ variable (simple linear regression)\n",
    "- When more than one $x$ variables (multiple regression), null hypothesis of the F-test is  \n",
    "__All $\\beta_i$ = 0 other than the intercept__.  \n",
    "That is, none of the preditors are useful. \n",
    "- t-test tests each $x$ variable individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqiD1GszIepg",
    "nbpresent": {
     "id": "f1c35822-c66e-4c87-b215-783c8e45c21b"
    }
   },
   "source": [
    "### How good is the fit? The $R^2$ value\n",
    "- coefficient of determination ($R^2$)\n",
    "    - Total sum of square \n",
    "    $SS_{tot}=\\sum_{i=1}^{n} (y_i-\\bar Y)^2$\n",
    "    - Regression sum of square:\n",
    "    $SS_{reg}=\\sum_{i=1}^{n} (\\hat y_i- \\bar Y)^2$\n",
    "    -  Sum of squares of residuals:\n",
    "    $SS_{res}=\\sum_{i=1}^{n} (\\hat y_i- y_i)^2$\n",
    "    - coefficient of determination \n",
    "    $R^2 = 1-\\frac{SS_{res}}{SS_{tot}} = 1-\\frac{\\sum_{i=1}^{n} (\\hat y_i- y_i)^2}{\\sum_{i=1}^{n} (y_i-\\bar Y)^2}$\n",
    "    - or $R^2 = \\frac{SS_{reg}}{SS_{tot}} = \\frac{\\sum_{i=1}^{n} (\\hat y_i- \\bar y)^2}{\\sum_{i=1}^{n} (y_i-\\bar Y)^2}$\n",
    "    - Close to 1 means a good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBYwOzHkIepg"
   },
   "source": [
    "### A Small example ###\n",
    "- $\\{x_1,x_2,x_3,x_4\\} = \\{5,5,10,10\\}$\n",
    "- $\\{y_1,y_2,y_3,y_4\\} = \\{14,17,27,22\\}$\n",
    "- $\\bar X = (5+5+10+10)/4=7.5$\n",
    "- $\\bar Y = (14+17+27+22)/4=20$\n",
    "- $\\hat\\beta_1 = \\frac{\\sum_{i=1}^{n}(x_i-\\bar X)(y_i-\\bar Y)}{\\sum_{i=1}^{n}(x_i-\\bar X)^2}\n",
    "=\\frac{(5-7.5)*(14-20)+(5-7.5)*(17-20)+(10-7.5)*(27-20)+(10-7.5)*(22-20)}\n",
    "{(5-7.5)^2+(5-7.5)^2+(10-7.5)^2+(10-7.5)^2}=\\frac{45}{25}=1.8$\n",
    "- $\\hat\\beta_0 = \\bar Y - \\hat\\beta_1 \\bar X = 20 - 1.8*7.5 = 6.5$\n",
    "- $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1*x_i = \\{15.5, 15.5, 24.5, 24.5\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwTrPruDIeph"
   },
   "source": [
    "#### estimate of the variance ####\n",
    "- **MSE**: $S^2 = \\frac{\\sum_{i=1}^{n} (y_i-\\hat y_i)^2}{n-2}=\n",
    "\\frac{(14-15.5)^2+(17-15.5)^2+(27-24.5)^2+(22-24.5)^2}{4-2}=8.5$\n",
    "- $S = \\sqrt{8.5}=2.92$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXV3oSuaIepi"
   },
   "source": [
    "####  confidence intervals ####\n",
    "- **Standard errors**:\n",
    "\n",
    "$$SE(\\hat\\beta_0) = S\\left(\\frac{1}{n}+\\frac{\\bar X^2}{\\sum_{i=1}^{n}(x_i-\\bar X)^2}\\right)^{1/2}\n",
    "= 2.92*(1/4+7.5^2/25)^{1/2}=4.61 $$\n",
    "  \n",
    "$SE(\\hat\\beta_1) = \\frac{S}{\\left(\\sum_{i=1}^{n}(x_i-\\bar X)^2\\right)^{1/2}}\n",
    "= 2.92/\\sqrt{25}=0.58$\n",
    "- To find 95% confidence intervals for $\\hat\\beta_0$ and $\\hat\\beta_1$:\n",
    "\n",
    "$t(0.05)_{df=4-2=2}=4.3$\n",
    "    \n",
    "$ CF(\\hat\\beta_0) = \\beta_0\\pm t(0.05)*SE(\\hat\\beta_0)= 6.5\\pm 4.3*4.61 = [-13.33,26.33] $\n",
    "\n",
    "$CF(\\hat\\beta_1) = \\beta_1\\pm t(0.05)*SE(\\hat\\beta_1)=1.8\\pm 4.3*0.58=[-0.71, 4.31]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_m3e8L2Iepi"
   },
   "source": [
    "#### ANOVA ###\n",
    "- Total sum of square: $SS_{tot}=\\sum_{i=1}^{n} (y_i-\\bar Y)^2 = (14-20)^2+(17-20)^2+(27-20)^2+(22-20)^2 = 98$\n",
    "- Regression sum of square: $SS_{reg}=\\sum_{i=1}^{n} (\\hat y_i- \\bar Y)^2\n",
    "=(15.5-20)^2+(15.5-20)^2+(24.5-20)^2+(24.5-20)^2=81$\n",
    "-  Sum of squares of residuals: $SS_{res}=\\sum_{i=1}^{n} (\\hat y_i- y_i)^2\n",
    "=(15.5-14)^2+(15.5-17)^2+(24.5-27)^2+(24.5-22)^2=17$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnW8fIwAIepj",
    "nbpresent": {
     "id": "0ddb0ca0-4b5e-4d49-8455-4292085fe4d0"
    }
   },
   "source": [
    "#### How good is the fit $R^2$ ###\n",
    "- $R^2 = 1-\\frac{SS_{res}}{SS_{tot}}=1-\\frac{17}{98}=0.83$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVGEv9vqIepj",
    "nbpresent": {
     "id": "dcac7889-4216-42ea-ac17-a43657a9511d"
    }
   },
   "source": [
    "\n",
    "#### Hypothesis testing on the small example ####\n",
    "- Suppose the null hypothesis is $\\beta_0^0=0$ and $\\beta_1^0=0$\n",
    "- $t(\\beta_0) = \\frac{\\beta_0-\\beta_0^0}{SE(\\beta_1)}=(6.5-0)/4.61=1.41$. $p=0.29$. So null hypothesis is not rejected ($\\beta_0$ may be zero)\n",
    "- $t(\\beta_1) = \\frac{\\beta_1-\\beta_1^0}{SE(\\beta_2)}=(1.8-0)/0.58=3.09$. $p=0.09$. So null hypothesis is not rejected ($\\beta_1$ may be zero)\n",
    "- $F = \\frac{SS_{reg}/1}{SS_{res}/(n-2)} = (81/1)/(17/(4-2))= 9.53.$ $p=0.09.$ Null hypothesis $\\beta_1=0$ is not rejected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtmB12OoIepk",
    "nbpresent": {
     "id": "cb8aad07-a422-49de-b7b1-d69048b762c6"
    }
   },
   "outputs": [],
   "source": [
    "df = 4 - 2\n",
    "2 * (1 - stats.t.cdf(1.41, df)), 2 * (1 - stats.t.cdf(3.09, df)), (\n",
    "    1 - stats.f.cdf(9.53, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBHxrp4iUmVA"
   },
   "source": [
    "Note that the p-values for the t-test and F-test are the same under simple linear regression.   \n",
    "When $X$ ~ t(n-2), then $X^2$ ~ F(1, n-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLaTXuUKIepm",
    "nbpresent": {
     "id": "6bcf8f74-bf47-4a05-9ac9-344202d30035"
    }
   },
   "source": [
    "### Confirming our hand calculation with Python ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1634084972889,
     "user": {
      "displayName": "Feng Mai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2Mksei3P2TEypMYoSXEhyCfsnGcI1ZVgH3cdbqw=s64",
      "userId": "07316652680205976084"
     },
     "user_tz": 240
    },
    "id": "3LXNGZJkIepn",
    "nbpresent": {
     "id": "e579453f-517b-45d2-a6f3-5d8284d0b926"
    },
    "outputId": "05137b53-483b-472e-a26c-1de3399adf7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Dec 2022</td> <th>  Prob (F-statistic):</th>  <td>0.0909</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:40:29</td>     <th>  Log-Likelihood:    </th> <td> -8.5696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     4</td>      <th>  AIC:               </th> <td>   21.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     2</td>      <th>  BIC:               </th> <td>   19.91</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    6.5000</td> <td>    4.610</td> <td>    1.410</td> <td> 0.294</td> <td>  -13.334</td> <td>   26.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.8000</td> <td>    0.583</td> <td>    3.087</td> <td> 0.091</td> <td>   -0.709</td> <td>    4.309</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   2.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   0.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.000</td> <th>  Prob(JB):          </th> <td>   0.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.221</td> <th>  Cond. No.          </th> <td>    25.4</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.827\n",
       "Model:                            OLS   Adj. R-squared:                  0.740\n",
       "Method:                 Least Squares   F-statistic:                     9.529\n",
       "Date:                Mon, 12 Dec 2022   Prob (F-statistic):             0.0909\n",
       "Time:                        13:40:29   Log-Likelihood:                -8.5696\n",
       "No. Observations:                   4   AIC:                             21.14\n",
       "Df Residuals:                       2   BIC:                             19.91\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          6.5000      4.610      1.410      0.294     -13.334      26.334\n",
       "x1             1.8000      0.583      3.087      0.091      -0.709       4.309\n",
       "==============================================================================\n",
       "Omnibus:                          nan   Durbin-Watson:                   2.059\n",
       "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.527\n",
       "Skew:                          -0.000   Prob(JB):                        0.768\n",
       "Kurtosis:                       1.221   Cond. No.                         25.4\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "xsmall = [5, 5, 10, 10]\n",
    "ysmall = [14, 17, 27, 22]\n",
    "xsmall2 = sm.add_constant(xsmall)\n",
    "est_small = sm.OLS(ysmall, xsmall2).fit()\n",
    "est_small.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9vMQETTIepr"
   },
   "source": [
    "### Diagnosis: Examine the residule plot ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTjYMOaEIeps"
   },
   "source": [
    "- Plot of residuals (y axis) against X can show us the linearrity of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKSYhUBgUmVI"
   },
   "source": [
    "- It can also show if $\\epsilon_i$ are not iid:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N7fciCiUmVI"
   },
   "source": [
    "<img src=\"http://3.bp.blogspot.com/-yJygqvJgMK8/UJXjAZymkqI/AAAAAAAAE-Q/vEUKq95msSE/s1600/2012-04-heteroskedasticity_modelling.png\" width=\"600px\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mo0XIIrIepv"
   },
   "source": [
    "### Confidence interval for a particular prediction ###\n",
    "- Confidence interval for a particular prediction \n",
    "- $\\hat y_i = \\hat\\beta_0 \\pm \\hat\\beta_1 x_i$ \n",
    "- CI at $x_i$ is\n",
    "$$\\hat y_i \\pm t_{n-2} S\\sqrt{1+h_i}$$\n",
    "- Here $h_i = \\frac{1}{n}+\\frac{(x_i-\\bar X)}{\\sum_{j=1}^n\\left(x_j - \\bar X \\right)^2}$, so the further away is $x_i$ from $\\bar x$, the looser the prediction\n",
    "- $[-t_{n-2}, t_{n-2}]$ is the 95% CI of t-distribution with $df=n-2$\n",
    "- When using a regression model to predict, one should not go beyond the range of observed data\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "8imkI2v7IepN",
    "DqiD1GszIepg",
    "xwTrPruDIeph",
    "yXV3oSuaIepi",
    "L_m3e8L2Iepi",
    "CnW8fIwAIepj"
   ],
   "name": "Simple_Linear_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "3ded31ea-4d42-48d1-b05d-e7a4411858b1",
    "theme": {
     "3ded31ea-4d42-48d1-b05d-e7a4411858b1": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "3ded31ea-4d42-48d1-b05d-e7a4411858b1",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "601.989px",
    "width": "502.003px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
