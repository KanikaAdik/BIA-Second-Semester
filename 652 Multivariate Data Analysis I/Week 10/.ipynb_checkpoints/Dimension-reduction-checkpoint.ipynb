{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGPbxoE91WsZ"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VExSM8GR1Wsh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cereal = pd.read_csv(\"http://fengmai.net/download/data/bia652/cereal.csv\", sep=\" \")\n",
    "names = cereal[\"name\"].values\n",
    "cereal = cereal.drop([\"name\",\"mfr\",\"type\"],1)\n",
    "cereal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OalpTY0j-AQ"
   },
   "outputs": [],
   "source": [
    "cereal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tX8uTGMb93B"
   },
   "outputs": [],
   "source": [
    "cereal.head(10) # we can see that there are missing values! (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkXHju8j1Wsk"
   },
   "source": [
    "- We replace missing values with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCxQUBaDb93F"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "imputer = imputer.fit(cereal)\n",
    "cereal[:] = imputer.transform(cereal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06_K_TPtb93K"
   },
   "outputs": [],
   "source": [
    "cereal.head(10) # we can see that there are missing values! (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-j8RrZV1Wsl"
   },
   "outputs": [],
   "source": [
    "# alternative method\n",
    "for vals in cereal.columns:\n",
    "    c = cereal[vals]\n",
    "    avg = np.mean(c[c != -1])\n",
    "    cereal[vals] = c.replace(-1, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qm9a3D7b93P"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "cereal2 = StandardScaler(with_std=True).fit_transform(cereal) #standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnwO7ZBMlk5U"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cereal2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEK6XuvYb93S"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLNsjJP31Ws5"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, svd_solver = 'full') #we initialize y with PCA. \n",
    "pca.fit(cereal2)\n",
    "cereal_pca = pca.transform(cereal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lQKs8Hom_0i"
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7YStlm_1Ws7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate 77 random colors, one for each cereal\n",
    "import random\n",
    "random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = [\"#%06x\" % random.randint(0, 0xAAAAAA) for i in range(0, cereal.shape[0])]\n",
    "# image size\n",
    "from pylab import rcParams #set figure size\n",
    "rcParams['figure.figsize'] = 15, 15\n",
    "\n",
    "#scatter plot\n",
    "for x, y, c in zip(cereal_pca[:,0], cereal_pca[:,1], color):\n",
    "    plt.scatter(x,y,color=c)\n",
    "    \n",
    "#labels\n",
    "for label, x, y, c in zip(names, cereal_pca[:,0],cereal_pca[:,1], color):\n",
    "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeXOJ0R4b93a"
   },
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttuRU7kDb93a"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2, algorithm='arpack') #we initialize y with PCA. \n",
    "svd.fit(cereal2)\n",
    "cereal_svd = svd.fit_transform(cereal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prSfBrIXb93d"
   },
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "for x, y, c in zip(cereal_svd[:,0], cereal_svd[:,1], color):\n",
    "    plt.scatter(x,y,color=c)\n",
    "    \n",
    "#labels\n",
    "for label, x, y, c in zip(names, cereal_svd[:,0],cereal_svd[:,1], color):\n",
    "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqgpRUTBb93f"
   },
   "source": [
    "# Multidimensional Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgQhZvfp1Wsa"
   },
   "source": [
    "## Motivation\n",
    "- In PCA, we project our $n\\times k$ data matrix $X$ to $n\\times l$. E.g., $l = 2,3$ can be used for visualization.\n",
    "- This projection is linear, and is done to maximize the variation preserved.\n",
    "- There is no guarantee that two data points (two rows of $X$) that are far away in the $k$ dimensional space get projected to be very close\n",
    "- Can we preserve the distance in $k$-dimensional space after \"projecting\" to $l$-dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmmPxk3T1Wsb"
   },
   "source": [
    "- E.g., we want the distance between any two points on the left to be similar to the corresponding distance on the right\n",
    "\n",
    "| $k$-Dimensional  | $l$-Dimensional  |\n",
    "|---|---|\n",
    "| <img width=\"300px\" src=\"http://fengmai.net/download/data/bia652/images/mds1.png\"></img>  |  <img  width=\"300px\" src=\"http://fengmai.net/download/data/bia652/images/mds2.png\"></img>   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAjpLsDT1Wsc"
   },
   "source": [
    "- We can achieve this by minimizing the \"stress\"\n",
    "- Let $d_{ij}=\\|x_i-x_j\\|$ be the distance between points $i$ and $j$ in $k$-D\n",
    "- Let point $i,\\ j$ be projected to $y_i,\\ y_j$ in $l$-D\n",
    "- We minimize\n",
    "\n",
    "$$\\text{stress} = \\sum_{i\\neq j} w_{\\text{ij}} \\left(\\left\\|y_i - y_j\\right\\| - d_{\\text{ij}}\\right){}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prANWh6M1Wsc"
   },
   "source": [
    "- Here $w_{ij}$ are the weights. E.g., if $w_{ij}=\\frac{1}{d_{ij}^2}$, then\n",
    "\n",
    "$$\\text{stress} = \\sum _{i\\neq j} \\left(\\frac{\\left\\|y_i - y_j\\right\\|}{d_{\\text{ij}}} - 1\\right)^2,$$\n",
    "\n",
    "- thus stress measures the relative difference between the actual edge length and ideal edge length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHpP6cVc1Wsd"
   },
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ieqIULw1Wsn"
   },
   "source": [
    "- set colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYRw283k1Wsq"
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=123,\n",
    "                   dissimilarity=\"euclidean\", n_jobs=1)\n",
    "\n",
    "cereal_mds = mds.fit(cereal2).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3IvgoQz1Ws1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "for x, y, c in zip(cereal_mds[:,0], cereal_mds[:,1], color):\n",
    "    plt.scatter(x,y,color=c)\n",
    "    \n",
    "#labels\n",
    "for label, x, y, c in zip(names, cereal_mds[:,0],cereal_mds[:,1], color):\n",
    "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O68l1y2c1Ws-"
   },
   "source": [
    "# SNE and T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f24J3kbF1Ws-"
   },
   "source": [
    "- We can achieve a similar goal by first define the likelyhood of being \"neighbors\"\n",
    "- We know the the closer two points are, the more likely they will be neighbors\n",
    "- Let $P(j|i)$ defines the probablity of $j$ being a neighbor of $i$.\n",
    "- In the original $k$-Dimensional space, this should be inverse proportional to $\\|x_i - x_j\\|$\n",
    "- So we could set\n",
    "\n",
    "$$P(j|i) = e^{-\\frac{\\|x_i-x_j\\|}{2\\sigma_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBtCI5TOb93s"
   },
   "source": [
    "- The above function is also called Gaussian or RBF kernel. $\\sigma$ is a hyperparameter determined later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctCTGV_M1Ws_"
   },
   "source": [
    "- But we want that probability for all $j\\ne i$ to sum to 1.\n",
    "- So we normalize and redefine\n",
    "$$p_{ij} = P(j|i) = \\frac{e^{-\\frac{\\|x_i-x_j\\|}{2\\sigma_i^2}}}{\\sum_{k\\ne i} e^{-\\frac{\\|x_i-x_k\\|}{2\\sigma_i^2}}}$$\n",
    "- The sequence $\\{p_{ij}\\ |\\ j \\ne i\\}$ form a  probablity distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnEbKTES1WtA"
   },
   "source": [
    "- Now after \"projecting\" to the lower dimensional space, we can also define similarly the probability of $i$ and $j$ being neighbors\n",
    "\n",
    "$$q_{ij} = \\frac{e^{-\\|y_i-y_j\\|^2}}{\\sum_{k\\ne i} e^{-\\|y_i-y_k\\|^2}}$$\n",
    "\n",
    "- Here we omitted the \"$\\sigma$\", since they can be part of $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzCjmM5H1WtA"
   },
   "source": [
    "- We want the two distribution to be similar. i.e., say $i=10$, and data points $1,\\ 5,\\ 12$ are close to $10$ so $p_{10,1},\\ p_{10,5},\\ p_{10,12}$ are relatively large compared with other points\n",
    "\n",
    "- So we want find $y$ such that $q_{10,1},\\ q_{10,5},\\ q_{10,12}$ are also relatively large\n",
    "\n",
    "- The standard way to measure the difference of two distributions is KL divergence\n",
    "\n",
    "$$KL(p_i, q_i) = \\sum_{k \\ne i} p_{ij} log\\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "- This measures the distance between two distributions\n",
    "\n",
    "- So we want to minimize $y_i$ such that  \n",
    "\n",
    "$$C(y_1, y_2, \\ldots, y_n) = \\sum_i KL(p_i, q_i) = \\sum_i \\sum_{k \\ne i} p_{ij} log\\frac{p_{ij}}{q_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6vlIbTd1WtB"
   },
   "source": [
    "- The objective is a nonlinear function of $y$.\n",
    "- The way to minimize the objective function is via stochastic gradient descent\n",
    "- The gradient of $C$ with regard to $y_i$ is \n",
    "$$\\frac{\\partial C}{\\partial y_i} = 2\\sum_j (y_i-y_j)(p_{ij} - q_{ij} + p_{ji} - q_{ji})$$\n",
    "\n",
    "- Then $y_i = y_i - t \\frac{\\partial C}{y_i}$\n",
    "- We will use sklearn TSNE package\n",
    "- TSNE is similar to SNE, except that \n",
    "$q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k\\ne l} (1 + \\|y_l - y_k\\|^2)^{-1} }$\n",
    "- $(1 + \\|y_i - y_j\\|^2)^{-1}$ decays slower than $e^{-\\|y_i-y_j\\|}$ as distance increases, avoid some of the overcrowding problems with SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Fsth-nw1WtB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# perplexity determines sigma\n",
    "model = TSNE(n_components=2, random_state = 1, perplexity=50, method=\"exact\", n_iter = 1000, learning_rate = 100)\n",
    "cereal_tsne = model.fit_transform(cereal2) \n",
    "\n",
    "#scatter plot\n",
    "for x, y, c in zip(cereal_tsne[:,0], cereal_tsne[:,1], color):\n",
    "    plt.scatter(x,y,color=c)\n",
    "#labels\n",
    "for label, x, y, c in zip(names, cereal_tsne[:,0],cereal_tsne[:,1], color):\n",
    "    plt.annotate(label, xy = (x, y), xytext = (-0, 0),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom', color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7akwl7Wsb93x"
   },
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-Fd9wRWb93y"
   },
   "source": [
    "<img width=\"200px\" src=\"http://fengmai.net/download/data/bia652/images/text.gif\" style=\"float: left; width: 45%; margin-right: 1%; margin-bottom: 0.5em;\"></img><img width=\"200px\" src=\"http://fengmai.net/download/data/bia652/images/text2vectors.png\" style=\"float: right; width: 45%; margin-right: 1%; margin-bottom: 0.5em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSiPAdcHb93y"
   },
   "source": [
    "- In text, each word is a categorical feature\n",
    "- If your text uses a dictionary of size 100K, you have 100K features\n",
    "- word2vec represent each word by a vector of dimension $k$\n",
    "- reduce the dimensional from 100K to $k$\n",
    "- It is a dimensional reduction applied to a sequence of co-occuring tokens (words)\n",
    "- SVD/PCA can be slow with a large matrix ($O(p^2 n+p^3)$)\n",
    "- How does it work? (Take BIA-667 or another DL/NLP course!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30l4qGo6b93z"
   },
   "source": [
    "## Python package\n",
    "- Python: Gensim [models.word2vec](https://radimrehurek.com/gensim/intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUNfDVsQb930"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "response = urllib.request.urlopen('http://fengmai.net/download/data/bia652/documents.txt')\n",
    "text_doc = response.read()\n",
    "Path(\"documents.txt\").write_text(str(text_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGRjD94E-RnI"
   },
   "outputs": [],
   "source": [
    "Path('documents.txt').read_text()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1668468014132,
     "user": {
      "displayName": "Feng Mai",
      "userId": "07316652680205976084"
     },
     "user_tz": 300
    },
    "id": "1XANtb5Eb935"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# train word2vec on your file\n",
    "model0 = Word2Vec(LineSentence(\"documents.txt\"), size=10, window=5, min_count=1, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F88pS1uCb937"
   },
   "source": [
    "- Look at a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1668468014132,
     "user": {
      "displayName": "Feng Mai",
      "userId": "07316652680205976084"
     },
     "user_tz": 300
    },
    "id": "ystzSBERb938",
    "outputId": "2859e7d3-4645-4da3-c5e2-c62cf7f72e22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.7787977 ,  2.9085257 , -0.1965623 , -1.6382704 ,  1.3658626 ,\n",
       "       -0.34135985, -2.5573366 ,  3.443796  ,  1.7005769 ,  2.5193603 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0['profit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjBu_BX4b93-"
   },
   "source": [
    "- Find closest to a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1668468014132,
     "user": {
      "displayName": "Feng Mai",
      "userId": "07316652680205976084"
     },
     "user_tz": 300
    },
    "id": "15h5TNFmb93-",
    "outputId": "b0105b87-14e2-4892-a8f6-14c61ab9cd17",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('revenue,', 0.9619590640068054),\n",
       " ('contribution', 0.9570902585983276),\n",
       " ('organic', 0.9567114114761353),\n",
       " ('income', 0.9562941789627075),\n",
       " ('loss', 0.9560518860816956),\n",
       " ('revenue', 0.9508347511291504),\n",
       " ('expenses', 0.9459767937660217),\n",
       " ('weighted', 0.9396394491195679),\n",
       " ('NOI', 0.9381172060966492),\n",
       " ('EBITDA', 0.936472475528717)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.most_similar('profit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o6UBNqWb94B"
   },
   "source": [
    "Using word vectors:\n",
    "\n",
    "1. As continuous features in linear/logistic regression\n",
    "2. finding similarity of words or sentences. vector for a sentence = sum/mean of vectors for the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K0IVlNVcAl1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1YaUANpBBCx7m68UYh7FdxBFMZMwuOqGf",
     "timestamp": 1668467893213
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
